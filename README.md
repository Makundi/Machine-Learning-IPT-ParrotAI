# Machine-Learning-IPT-ParrotAI

## Week1 Report

Here is a my short summary of what I have achieved to learn in my first week of training under ParrotAi.

Introduction to Machine learning , I have achieved to know a good intro into Machine Learning which include the history of ML ,the types of ML  such supervised, unsupervised, Reinforcement learning. And also answers to questions such why machine learning? , challenges facing machine learning which include insufficient data, irrelevant on data, overfitting, underfitting and there solutions in general.

Supervised Machine algorithms, here I learnt the theory and intuition  behind the common used supervised ML including the KNN, Linear Regressions, Logistic, Regression, and Ensemble algorithm the Random forest. Also not only the intuition but their implementation in python using the sklearn library and parameter tuning them to achieve a best model with stunning accuracy(here meaning the way to regularize the model to avoid overfitting and underfitting).And also the intuition on where to use/apply the algorithms basing on the problem I.e classification or regression. Also which model performs better on what and poor on what based on circumstances.

Data preprocessing and representation here I learnt on the importance of preprocessing the data, also the techniques involved such scaling(include Standard Scaling, RobustScaling  and MinMaxScaler) ,handling the missing data either by ignoring(technical termed as dropping) the data which is not recommended since one could loose important patterns on the data and by fitting the mean or median of the data points on the missing places. On data representation involved on how we can represent categorical features so as they can be used in the algorithm, the method learnt here is One-Hot Encoding technique and its implementation in python using both Pandas and Sklearn Libraries.

Model evaluation and improvement. In this section I grasped the concept of how you can evaluate your model if its performing good or bad and the ways you could improve it. As the train_test_split technique  seems to be imbalance hence the cross-validation technique which included the K-fold , Stratified K-fold and other strategies such LeaveOneOut which will help on the improvement of your model by splitting data in a convenience  manner  to help in training of model, thus making it generalize well on unseen data. I learnt also on the GridSearch technique which included the best method in which one can choose the best parameters for the model to improve the performance such as the simple grid search and the GridSearch with cross-validation technique, all this I was able to implement them in code using the Sklearn library in python.

Lastly the week challenge task given to us was tremendous since I got to apply what I learned in theory to solve a real problem.It was good to apply the workflow of a machine learning task starting from understanding the problem, getting to know the data, data preprocessing , visualising the data to get more insights, model selection, training the model  to applying the model to make prediction.

## Conclusion
In general it was a tuff week basing on the modules but all in all i was able to grasp and learn much in this week from basic foundation of Machine Learning to the implementations of the algorithms in code. The great achievement so far is the intuition behind the algorithm especially supervised ones. Though yet is much to be covered but the accomplishment I have attained so far its a good start to say to this journey on Machine learning. My expectation on the coming week is on having a solid foundation on deep learning.
#
